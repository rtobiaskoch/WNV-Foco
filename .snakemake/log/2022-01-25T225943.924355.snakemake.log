Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job          count    min threads    max threads
---------  -------  -------------  -------------
align            1              1              1
ancestral        1              1              1
export           1              1              1
filter           1              1              1
refine           1              1              1
traits           1              1              1
translate        1              1              1
tree             1              1              1
total            8              1              1

Select jobs to execute...

[Tue Jan 25 22:59:44 2022]
Job 4: 
        Filtering to
          - 20 sequence(s) per country year month
          - from 1900 onwards
          - excluding strains in config/dropped_strains.txt
        

[Tue Jan 25 22:59:44 2022]
Error in rule filter:
    jobid: 4
    output: results/filtered.fasta
    shell:
        
        augur filter             --sequences data/sequences.fasta             --metadata data/metadata.tsv             --exclude config/dropped_strains.txt             --output results/filtered.fasta             --group-by country year month             --sequences-per-group 20             --min-date 1900
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /Users/user/Programming Directory/Grubaugh Lab/WNV-nextstrain/.snakemake/log/2022-01-25T225943.924355.snakemake.log
